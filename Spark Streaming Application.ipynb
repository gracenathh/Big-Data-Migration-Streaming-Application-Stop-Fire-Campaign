{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming Application for Data from Kafka Producers\n",
    "\n",
    "The answers have been modified for portfolio purpose\n",
    "\n",
    "Author: Grace Nathania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash\n",
    "import datetime as dt\n",
    "\n",
    "def find_latest_date(col):\n",
    "    cursor = col.aggregate([{\"$sort\":{\"_id\":-1}}, {\"$limit\":1}])\n",
    "    \n",
    "    for doc in cursor:\n",
    "        date = doc['date']\n",
    "        date = dt.datetime.strptime(date,'%d/%m/%y').date() + dt.timedelta(days=1)\n",
    "        \n",
    "        return date.strftime('%d/%m/%y')\n",
    "    \n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.stopfire_db\n",
    "    collection = db.data\n",
    "    record_list = [[],[],[]]\n",
    "    \n",
    "    for record in iter:\n",
    "        # Recovering original data type and strucutre\n",
    "        value = eval(record[1])\n",
    "        key = int(record[0])\n",
    "\n",
    "        # Storing data into record_list based on which producer data comes from\n",
    "        record_list[key-1].append(value)\n",
    "        \n",
    "        \n",
    "    # Early termination if climate data doesn't exist\n",
    "    if record_list[0] == []:\n",
    "        pass\n",
    "\n",
    "    # Data processing\n",
    "    else:\n",
    "        climate_data = record_list[0][0]\n",
    "        climate_date = climate_data['date']\n",
    "        climate_data.pop('created_date')\n",
    "        climate_data.pop('producer')\n",
    "\n",
    "        # Compare climate and hotspot with precision 3\n",
    "        c_lat = climate_data['latitude']\n",
    "        c_long = climate_data['longitude']\n",
    "        climate_val = geohash.encode(c_lat, c_long, precision = 3)\n",
    "\n",
    "        hotspot_list = []\n",
    "        aqua_hotspot = record_list[1]\n",
    "        terra_hotspot = record_list[2]\n",
    "\n",
    "        # checking hotspot data from AQUA satelite\n",
    "        for i in range(len(aqua_hotspot)):\n",
    "            lat = aqua_hotspot[i]['latitude']\n",
    "            long = aqua_hotspot[i]['longitude']\n",
    "            aqua_val = geohash.encode(lat, long, precision = 3)\n",
    "\n",
    "            if aqua_val == climate_val:\n",
    "                hotspot_list.append(aqua_hotspot[i])\n",
    "\n",
    "        # checking hotspot data from TERRA satelite\n",
    "        for i in range(len(terra_hotspot)):\n",
    "            lat = terra_hotspot[i]['latitude']\n",
    "            long = terra_hotspot[i]['longitude']\n",
    "            terra_val = geohash.encode(lat, long, precision = 3)\n",
    "\n",
    "            if terra_val == climate_val:\n",
    "                hotspot_list.append(terra_hotspot[i])\n",
    "                \n",
    "        # Setting station data on Climate with value of geohash to match the data in part A\n",
    "        climate_data['station'] = climate_val\n",
    "        \n",
    "        # Removing latitude and longitude data from climate_data \n",
    "        climate_data.pop(\"latitude\")\n",
    "        climate_data.pop(\"longitude\")\n",
    "\n",
    "        # no hotspot is near the climate data, thus only climate data is appended\n",
    "        if len(hotspot_list) == 0:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # Combine the hotspot data that are close to each other\n",
    "            hotspot_dict = {}\n",
    "\n",
    "            for data in hotspot_list:\n",
    "                lat = data['latitude']\n",
    "                long = data['longitude']\n",
    "                val = geohash.encode(lat,long,precision = 5)\n",
    "\n",
    "                if val in hotspot_dict.keys():\n",
    "                    dict_val = hotspot_dict[val]\n",
    "                    surf_dict = dict_val['surface_temperature_celcius']\n",
    "                    conf_dict = dict_val['confidence']\n",
    "                    lat_dict = dict_val['latitude']\n",
    "                    long_dict = dict_val['longitude']\n",
    "\n",
    "                    surf_data = data['surface_temperature_celcius']\n",
    "                    conf_data = data['confidence']\n",
    "\n",
    "                    surf_avg = (surf_dict + surf_data)/2\n",
    "                    conf_avg = (conf_dict + conf_data)/2\n",
    "                    lat_avg = (lat_dict + lat)/2\n",
    "                    long_avg = (long_dict + long)/2\n",
    "                    \n",
    "                    # Updating the current data with the average data\n",
    "                    hotspot_dict[val]['surface_temperature_celcius'] = surf_avg\n",
    "                    hotspot_dict[val]['confidence'] = conf_avg\n",
    "                    hotspot_dict[val]['latitude'] = lat_avg\n",
    "                    hotspot_dict[val]['longitude'] = long_avg\n",
    "\n",
    "                else:\n",
    "                    real_date = data['created_date'].split(\"T\")\n",
    "                    real_time = real_date[1]\n",
    "                    data['date_time'] = climate_date + \"T\" + real_time\n",
    "                    data['date'] = climate_date\n",
    "                    data.pop('created_date')\n",
    "                    data.pop('producer')\n",
    "                    hotspot_dict[val] = data\n",
    "\n",
    "            # Convert combined hotspots into list and store into climate data\n",
    "            hotspot_list = list(hotspot_dict.values())\n",
    "            climate_data['hotspot'] = hotspot_list\n",
    "\n",
    "            # Decide if the fire event is natural or other\n",
    "            if (climate_data['air_temperature_celcius'] > 20) and (climate_data['GHI_w/m2'] > 180):\n",
    "                climate_data['fire_cause'] = 'natural'\n",
    "            else:\n",
    "                climate_data['fire_cause'] = 'other'\n",
    "        \n",
    "        try:\n",
    "            collection.insert(climate_data)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                           \n",
    "    client.close()\n",
    "    \n",
    "n_secs = 10\n",
    "topic = 'StopFire'\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'assignment-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "#time.sleep(11000)\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
